# ST-MT-002-002: Implement AI Service Layer with Multi-Model Routing and Generation Engine

## 1. Sub-Task Overview
- **Sub-Task ID:** ST-MT-002-002
- **Sub-Task Name:** Implement AI Service Layer with Multi-Model Routing and Generation Engine
- **Parent Task:** MT-002: AI-Powered Development Automation with Document Generation
- **Estimated Duration:** 6 hours
- **Implementation Type:** Code

## 2. Deliverable Specification
- **Primary Output:** AI service layer with multi-model routing, document generation engine, and rule-based automation
- **Code Location:** 
  - `cli/internal/adapters/secondary/ai/service.go` - Main AI service interface
  - `cli/internal/adapters/secondary/ai/models/` - Model-specific implementations
  - `cli/internal/adapters/secondary/ai/generator.go` - Document generation engine
  - `cli/internal/adapters/secondary/ai/prompts/` - Rule-based prompt templates
- **Technical Requirements:** HTTP clients, AI API integration, fallback routing, response caching, document generation, rule application
- **Interface Definition:** AIService interface with generation capabilities and model-specific implementations

## 3. Implementation Details
- **Step-by-Step Approach:**
  1. Create AI service interface and base structures
  2. Implement Claude Sonnet 4 client integration
  3. Implement Perplexity Sonar Pro client integration
  4. Implement OpenAI GPT-4o client integration
  5. Create multi-model routing with fallback logic
  6. Implement document generation engine with rule support
  7. Create prompt templates for each document type (PRD, TRD, tasks)
  8. Add response caching system
  9. Implement retry logic with exponential backoff
  10. Create AI response validation and parsing

- **Code Examples:**
  ```go
  // Enhanced AI service interface with generation capabilities
  type AIService interface {
      // Document parsing
      ParsePRD(prd *PRD) (*AIAnalysis, error)
      ParseDocument(doc *Document) (*AIAnalysis, error)
      
      // Document generation
      GeneratePRD(context *GenerationContext) (*PRD, error)
      GenerateTRD(prd *PRD, rule *Rule) (*TRD, error)
      GenerateMainTasks(trd *TRD, rule *Rule) ([]*MainTask, error)
      GenerateSubTasks(mainTask *MainTask, rule *Rule) ([]*SubTask, error)
      
      // Task generation from analysis
      GenerateTasks(analysis *AIAnalysis, context TaskContext) ([]*TaskSuggestion, error)
      
      // Utility functions
      SuggestComplexity(content string) (ComplexityEstimate, error)
      ValidateResponse(response string) error
      
      // Interactive generation
      StartInteractiveSession(docType DocumentType) (*InteractiveSession, error)
      ContinueSession(sessionID string, userInput string) (*SessionResponse, error)
  }
  
  // Generation context for document creation
  type GenerationContext struct {
      Type         DocumentType       `json:"type"`
      UserInputs   []string          `json:"user_inputs"`
      Rule         *Rule             `json:"rule"`
      ParentDoc    *Document         `json:"parent_doc,omitempty"`
      Repository   string            `json:"repository"`
      Metadata     map[string]interface{} `json:"metadata"`
  }
  
  // AI analysis result
  type AIAnalysis struct {
      ID             string              `json:"id"`
      PRDID          string              `json:"prd_id"`
      ModelUsed      string              `json:"model_used"`
      ProcessedAt    time.Time           `json:"processed_at"`
      Summary        string              `json:"summary"`
      KeyFeatures    []string            `json:"key_features"`
      TechnicalReqs  []string            `json:"technical_requirements"`
      Dependencies   []string            `json:"dependencies"`
      Complexity     ComplexityEstimate  `json:"complexity"`
      Sections       []ContentSection    `json:"sections"`
      Metadata       map[string]interface{} `json:"metadata"`
  }
  
  type ComplexityEstimate struct {
      Overall        string  `json:"overall"`        // low, medium, high
      Score          float64 `json:"score"`          // 0-10
      Factors        []string `json:"factors"`
      EstimatedHours int     `json:"estimated_hours"`
      Confidence     float64 `json:"confidence"`     // 0-1
  }
  
  type TaskSuggestion struct {
      Title          string   `json:"title"`
      Description    string   `json:"description"`
      Type           string   `json:"type"`           // feature, bugfix, enhancement, etc
      Priority       string   `json:"priority"`       // low, medium, high
      Complexity     string   `json:"complexity"`     // low, medium, high
      EstimatedHours int      `json:"estimated_hours"`
      Dependencies   []string `json:"dependencies"`
      Tags           []string `json:"tags"`
      Acceptance     []string `json:"acceptance_criteria"`
      TechnicalNotes string   `json:"technical_notes"`
  }
  
  // Multi-model AI service implementation
  type MultiModelAIService struct {
      models     []AIModel
      cache      ResponseCache
      logger     *slog.Logger
      config     *AIConfig
      fallbackEnabled bool
  }
  
  type AIConfig struct {
      PrimaryModel    string        `mapstructure:"primary_model"`
      FallbackModels  []string      `mapstructure:"fallback_models"`
      MaxRetries      int           `mapstructure:"max_retries"`
      CacheEnabled    bool          `mapstructure:"cache_enabled"`
      CacheTTL        time.Duration `mapstructure:"cache_ttl"`
      RequestTimeout  time.Duration `mapstructure:"request_timeout"`
  }
  
  // Individual AI model interface
  type AIModel interface {
      Name() string
      IsAvailable() bool
      ParsePRD(prd *PRD, prompt string) (*AIAnalysis, error)
      GenerateTasks(analysis *AIAnalysis, prompt string) ([]*TaskSuggestion, error)
      TestConnection() error
  }
  
  func NewMultiModelAIService(config *AIConfig, logger *slog.Logger) *MultiModelAIService {
      service := &MultiModelAIService{
          models:          make([]AIModel, 0),
          cache:          NewResponseCache(config.CacheTTL),
          logger:         logger,
          config:         config,
          fallbackEnabled: len(config.FallbackModels) > 0,
      }
      
      // Initialize models based on configuration
      service.initializeModels()
      
      return service
  }
  
  func (s *MultiModelAIService) initializeModels() {
      // Add Claude Sonnet 4
      if claudeModel := NewClaudeModel(s.config, s.logger); claudeModel != nil {
          s.models = append(s.models, claudeModel)
      }
      
      // Add Perplexity Sonar Pro
      if perplexityModel := NewPerplexityModel(s.config, s.logger); perplexityModel != nil {
          s.models = append(s.models, perplexityModel)
      }
      
      // Add OpenAI GPT-4o
      if openaiModel := NewOpenAIModel(s.config, s.logger); openaiModel != nil {
          s.models = append(s.models, openaiModel)
      }
  }
  
  func (s *MultiModelAIService) ParsePRD(prd *PRD) (*AIAnalysis, error) {
      // Check cache first
      if s.config.CacheEnabled {
          if cached := s.cache.Get(prd.ContentHash); cached != nil {
              s.logger.Debug("returning cached PRD analysis")
              return cached.(*AIAnalysis), nil
          }
      }
      
      prompt := s.buildPRDParsingPrompt(prd)
      
      // Try primary model first
      analysis, err := s.executeWithFallback(func(model AIModel) (interface{}, error) {
          return model.ParsePRD(prd, prompt)
      })
      
      if err != nil {
          return nil, fmt.Errorf("failed to parse PRD with all available models: %w", err)
      }
      
      result := analysis.(*AIAnalysis)
      
      // Cache successful result
      if s.config.CacheEnabled {
          s.cache.Set(prd.ContentHash, result)
      }
      
      return result, nil
  }
  
  func (s *MultiModelAIService) executeWithFallback(operation func(AIModel) (interface{}, error)) (interface{}, error) {
      var lastErr error
      
      // Try primary model first
      primaryModel := s.getPrimaryModel()
      if primaryModel != nil && primaryModel.IsAvailable() {
          result, err := s.executeWithRetry(primaryModel, operation)
          if err == nil {
              return result, nil
          }
          lastErr = err
          s.logger.Warn("primary model failed, trying fallbacks",
              slog.String("model", primaryModel.Name()),
              slog.Any("error", err))
      }
      
      // Try fallback models
      if s.fallbackEnabled {
          for _, model := range s.getFallbackModels() {
              if model.IsAvailable() {
                  result, err := s.executeWithRetry(model, operation)
                  if err == nil {
                      s.logger.Info("fallback model succeeded",
                          slog.String("model", model.Name()))
                      return result, nil
                  }
                  lastErr = err
                  s.logger.Warn("fallback model failed",
                      slog.String("model", model.Name()),
                      slog.Any("error", err))
              }
          }
      }
      
      return nil, fmt.Errorf("all models failed, last error: %w", lastErr)
  }
  
  func (s *MultiModelAIService) executeWithRetry(model AIModel, operation func(AIModel) (interface{}, error)) (interface{}, error) {
      var lastErr error
      
      for attempt := 0; attempt <= s.config.MaxRetries; attempt++ {
          result, err := operation(model)
          if err == nil {
              return result, nil
          }
          
          lastErr = err
          
          // Check if error is retryable
          if !s.isRetryableError(err) {
              break
          }
          
          if attempt < s.config.MaxRetries {
              delay := s.calculateBackoff(attempt)
              s.logger.Debug("retrying AI operation",
                  slog.String("model", model.Name()),
                  slog.Int("attempt", attempt+1),
                  slog.Duration("delay", delay))
              time.Sleep(delay)
          }
      }
      
      return nil, lastErr
  }
  
  // Document generation engine
  type DocumentGenerator struct {
      aiService  *MultiModelAIService
      ruleManager RuleManager
      promptBuilder *PromptBuilder
      logger     *slog.Logger
  }
  
  func NewDocumentGenerator(aiService *MultiModelAIService, ruleManager RuleManager, logger *slog.Logger) *DocumentGenerator {
      return &DocumentGenerator{
          aiService:    aiService,
          ruleManager:  ruleManager,
          promptBuilder: NewPromptBuilder(ruleManager),
          logger:       logger,
      }
  }
  
  func (g *DocumentGenerator) GeneratePRD(context *GenerationContext) (*PRD, error) {
      // Get PRD generation rule
      rule, err := g.ruleManager.GetRule(RuleTypePRD)
      if err != nil {
          return nil, fmt.Errorf("failed to get PRD rule: %w", err)
      }
      
      // Build generation prompt from rule
      prompt := g.promptBuilder.BuildPRDGenerationPrompt(rule, context)
      
      // Execute generation with AI
      response, err := g.aiService.executeGeneration(prompt, "PRD")
      if err != nil {
          return nil, fmt.Errorf("failed to generate PRD: %w", err)
      }
      
      // Parse response into PRD
      prd := &PRD{
          Document: Document{
              ID:          uuid.New().String(),
              Type:        DocumentTypePRD,
              Name:        response.Title,
              Content:     response.Content,
              Format:      FormatMarkdown,
              Repository:  context.Repository,
              GeneratedBy: response.ModelUsed,
              CreatedAt:   time.Now(),
              UpdatedAt:   time.Now(),
          },
      }
      
      // Extract sections from generated content
      prd.Sections = g.extractPRDSections(response.Content)
      
      return prd, nil
  }
  
  func (g *DocumentGenerator) GenerateTRD(prd *PRD, rule *Rule) (*TRD, error) {
      // Build TRD generation prompt
      prompt := g.promptBuilder.BuildTRDGenerationPrompt(rule, prd)
      
      // Execute generation
      response, err := g.aiService.executeGeneration(prompt, "TRD")
      if err != nil {
          return nil, fmt.Errorf("failed to generate TRD: %w", err)
      }
      
      // Create TRD from response
      trd := &TRD{
          Document: Document{
              ID:          uuid.New().String(),
              Type:        DocumentTypeTRD,
              Name:        fmt.Sprintf("TRD for %s", prd.Name),
              Content:     response.Content,
              Format:      FormatMarkdown,
              Repository:  prd.Repository,
              GeneratedBy: response.ModelUsed,
              ParentDoc:   prd.ID,
              CreatedAt:   time.Now(),
              UpdatedAt:   time.Now(),
          },
          PRDReference: prd.ID,
      }
      
      // Extract technical details
      trd.TechStack = g.extractTechStack(response.Content)
      trd.Architecture = g.extractArchitecture(response.Content)
      
      return trd, nil
  }
  
  // Interactive session support
  type InteractiveSession struct {
      ID           string              `json:"id"`
      Type         DocumentType        `json:"type"`
      Rule         *Rule               `json:"rule"`
      State        SessionState        `json:"state"`
      Context      map[string]interface{} `json:"context"`
      Messages     []SessionMessage    `json:"messages"`
      CreatedAt    time.Time           `json:"created_at"`
      UpdatedAt    time.Time           `json:"updated_at"`
  }
  
  type SessionState string
  const (
      SessionStateActive    SessionState = "active"
      SessionStateCompleted SessionState = "completed"
      SessionStateCancelled SessionState = "cancelled"
  )
  
  func (s *MultiModelAIService) StartInteractiveSession(docType DocumentType) (*InteractiveSession, error) {
      // Get rule for document type
      rule, err := s.ruleManager.GetRule(RuleType(docType))
      if err != nil {
          return nil, fmt.Errorf("failed to get rule for %s: %w", docType, err)
      }
      
      session := &InteractiveSession{
          ID:        uuid.New().String(),
          Type:      docType,
          Rule:      rule,
          State:     SessionStateActive,
          Context:   make(map[string]interface{}),
          Messages:  []SessionMessage{},
          CreatedAt: time.Now(),
          UpdatedAt: time.Now(),
      }
      
      // Store session
      s.sessionStore.Save(session)
      
      // Generate initial prompt based on rule
      initialPrompt := s.promptBuilder.BuildInteractiveStartPrompt(rule, docType)
      
      // Get initial AI response
      response, err := s.executeWithFallback(func(model AIModel) (interface{}, error) {
          return model.GenerateInteractive(initialPrompt)
      })
      
      if err != nil {
          return nil, fmt.Errorf("failed to start interactive session: %w", err)
      }
      
      // Add to session messages
      session.Messages = append(session.Messages, SessionMessage{
          Role:      "assistant",
          Content:   response.(string),
          Timestamp: time.Now(),
      })
      
      return session, nil
  }
  
  func (s *MultiModelAIService) buildPRDParsingPrompt(prd *PRD) string {
      return fmt.Sprintf(`
  Analyze the following Product Requirements Document (PRD) and extract key information for task generation.
  
  Document Details:
  - Name: %s
  - Format: %s
  - Word Count: %d
  - Sections: %d
  
  Content:
  %s
  
  Please provide:
  1. A concise summary of the document
  2. Key features and requirements
  3. Technical requirements and constraints
  4. Dependencies and external requirements
  5. Complexity assessment with reasoning
  6. Major sections with their purposes
  
  Return the response in JSON format matching the AIAnalysis structure.
  `, prd.Name, prd.Format, prd.Metadata.WordCount, prd.Metadata.SectionCount, prd.Content)
  }
  
  // Response caching
  type ResponseCache interface {
      Get(key string) interface{}
      Set(key string, value interface{})
      Clear()
  }
  
  type InMemoryCache struct {
      data   map[string]cacheItem
      mutex  sync.RWMutex
      ttl    time.Duration
  }
  
  type cacheItem struct {
      value     interface{}
      expiresAt time.Time
  }
  
  func NewResponseCache(ttl time.Duration) *InMemoryCache {
      cache := &InMemoryCache{
          data: make(map[string]cacheItem),
          ttl:  ttl,
      }
      
      // Start cleanup goroutine
      go cache.cleanup()
      
      return cache
  }
  
  func (c *InMemoryCache) Get(key string) interface{} {
      c.mutex.RLock()
      defer c.mutex.RUnlock()
      
      item, exists := c.data[key]
      if !exists || time.Now().After(item.expiresAt) {
          return nil
      }
      
      return item.value
  }
  
  func (c *InMemoryCache) Set(key string, value interface{}) {
      c.mutex.Lock()
      defer c.mutex.Unlock()
      
      c.data[key] = cacheItem{
          value:     value,
          expiresAt: time.Now().Add(c.ttl),
      }
  }
  ```

- **Configuration Changes:** Adds AI service configuration to main config
- **Dependencies:**
  - HTTP client libraries for each AI service
  - JSON parsing and validation
  - Concurrent access control (sync)

## 4. Acceptance Criteria
- **Functional Criteria:**
  - Multi-model routing works with primary and fallback models
  - AI service generates complete PRDs, TRDs, main tasks, and sub-tasks
  - Document generation follows rule templates accurately
  - Interactive sessions guide users through document creation
  - Task generation creates relevant, actionable suggestions
  - Response caching reduces redundant API calls
  - Fallback system activates when primary model fails
  - Generated documents match expected structure and quality
  
- **Technical Criteria:**
  - Document generator integrates with rule manager
  - Prompt builder creates effective prompts from rules
  - Interactive sessions maintain state correctly
  - Retry logic handles transient failures gracefully
  - Response validation ensures data quality
  - Error handling provides clear model failure information
  - Configuration supports flexible model selection
  
- **Integration Criteria:**
  - AI service integrates with all document entities
  - Rule-based generation produces consistent results
  - Generated documents follow PRD/TRD/task formats
  - Cache system reduces AI API costs effectively
  - Interactive sessions work with REPL mode
  
- **Test Criteria:**
  - All AI models tested with mock responses
  - Document generation tested for all types
  - Rule application verified for consistency
  - Fallback logic verified under failure scenarios
  - Response caching accuracy validated

## 5. Testing Requirements
- **Unit Tests:**
  - Multi-model routing logic and fallback behavior
  - Document generation for all types (PRD, TRD, tasks)
  - Rule integration and prompt building
  - Interactive session state management
  - Response caching with TTL expiration
  - Retry logic with various error types
  - AI response validation and parsing
  - Prompt generation from rules
  - Task suggestion formatting and validation
  - Error handling for API failures

- **Integration Tests:**
  - End-to-end document generation workflow
  - Rule-based prompt generation accuracy
  - Interactive session flow with user inputs
  - Multi-model fallback during generation
  - Cache integration with real AI responses
  - Configuration loading and model initialization
  
- **Manual Testing:**
  - Real AI API integration with each model
  - Document quality assessment for generated content
  - Interactive PRD creation flow
  - Fallback behavior under rate limiting
  - Response quality assessment
  
- **Test Data:** Mock AI responses, sample documents, rule templates, error scenarios

## 6. Definition of Done
- **Code Complete:** AI service layer and document generator fully implemented
- **Tests Passing:** All generation and routing scenarios tested (≥85% coverage)
- **Documentation Updated:** AI service, generation API, and rule integration documented
- **Integration Verified:** Works with all document types and rule system
- **Review Approved:** Architecture, API security, and prompt injection prevention review

## 7. Dependencies and Blockers
- **Required Sub-Tasks:** ST-MT-002-001 (Document Entities and Rule Processing)
- **External Dependencies:** AI API access keys and service availability
- **Environmental Requirements:** Network access to AI services, rule templates
- **Potential Blockers:** AI API rate limits, prompt engineering complexity, rule parsing

## 8. Integration Notes
- **Component Interfaces:** Used by all document generation commands and REPL mode
- **Data Flow:** 
  - Generation: Rule + Context → AI → Document → Storage
  - Analysis: Document → AI → Analysis → Task Suggestions
  - Interactive: User Input → AI → Response → Session Update
- **Error Handling:** Graceful fallback with detailed error reporting, prompt validation
- **Configuration Impact:** Requires AI API keys, model configuration, and rule paths

## 9. Current Implementation Status

**Status:** 🚧 PARTIALLY IMPLEMENTED

**What has been implemented:**
- ✅ Basic AI service structure in `/internal/ai/` with document generation capabilities
- ✅ Document generator with PRD and TRD generation templates
- ✅ Interactive question generation for PRD and TRD creation
- ✅ AI request/response structures with metadata support
- ✅ Basic prompt building and template system
- ✅ Document parsing for PRD and TRD entities
- ✅ Technical stack and architecture extraction
- ✅ Integration with rule manager
- ✅ Basic OpenAI client implementation in `/internal/embeddings/openai.go`

**What is missing:**
- ❌ Multi-model routing with Claude Sonnet 4, Perplexity Sonar Pro
- ❌ Fallback logic and circuit breaker implementation
- ❌ Response caching system
- ❌ Retry logic with exponential backoff
- ❌ Task generation from AI analysis
- ❌ Interactive session state management
- ❌ Complete workflow orchestration
- ❌ Comprehensive error handling and model-specific implementations

**Next steps:**
1. Implement multi-model AI service with routing and fallback
2. Add Claude Sonnet 4 and Perplexity integration
3. Implement response caching and retry mechanisms
4. Complete task generation engine
5. Add interactive session management
6. Implement comprehensive error handling

**Deviations from original plan:**
- AI service is implemented in main MCP server rather than CLI-specific adapters
- Document generator exists but multi-model routing is not implemented
- Basic structure is there but lacks the sophistication described in the original plan
- Focus has been on OpenAI integration rather than multiple AI providers