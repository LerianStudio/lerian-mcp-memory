# ST-MT-005-002: Setup Monitoring Stack with Prometheus and Grafana

## 1. Sub-Task Overview
- **Sub-Task ID:** ST-MT-005-002
- **Sub-Task Name:** Setup Monitoring Stack with Prometheus and Grafana
- **Parent Task:** MT-005: Production-Ready System with Advanced Features
- **Estimated Duration:** 3 hours
- **Implementation Type:** Code/Configuration

## 2. Deliverable Specification
- **Primary Output:** Complete observability stack with metrics collection, dashboards, and alerting
- **Code Location:** 
  - `internal/monitoring/metrics.go` - Metrics instrumentation
  - `internal/monitoring/collectors.go` - Custom Prometheus collectors
  - `deployments/docker/prometheus/` - Prometheus configuration
  - `deployments/docker/grafana/` - Grafana dashboards
  - `cli/internal/adapters/secondary/metrics/exporter.go` - CLI metrics
- **Technical Requirements:** Prometheus metrics, Grafana dashboards, Docker Compose setup, alerting rules
- **Interface Definition:** Metrics endpoints, dashboard definitions, alert configurations

## 3. Implementation Details
- **Step-by-Step Approach:**
  1. Instrument server code with Prometheus metrics
  2. Create custom collectors for business metrics
  3. Setup Prometheus configuration and scraping
  4. Design Grafana dashboards for key metrics
  5. Implement CLI metrics collection
  6. Configure alerting rules
  7. Create Docker Compose monitoring stack
  8. Add health check endpoints
  9. Implement SLI/SLO tracking
  10. Create runbooks for alerts

- **Code Examples:**
  ```go
  // Metrics instrumentation
  package monitoring
  
  import (
      "github.com/prometheus/client_golang/prometheus"
      "github.com/prometheus/client_golang/prometheus/promauto"
      "github.com/prometheus/client_golang/prometheus/promhttp"
  )
  
  var (
      // Task metrics
      tasksTotal = promauto.NewCounterVec(
          prometheus.CounterOpts{
              Name: "lmmc_tasks_total",
              Help: "Total number of tasks created",
          },
          []string{"repository", "priority", "status"},
      )
      
      taskDuration = promauto.NewHistogramVec(
          prometheus.HistogramOpts{
              Name:    "lmmc_task_duration_seconds",
              Help:    "Task completion duration in seconds",
              Buckets: prometheus.ExponentialBuckets(60, 2, 10), // 1min to ~17hrs
          },
          []string{"repository", "priority"},
      )
      
      activeTasksGauge = promauto.NewGaugeVec(
          prometheus.GaugeOpts{
              Name: "lmmc_active_tasks",
              Help: "Number of active tasks",
          },
          []string{"repository", "status"},
      )
      
      // API metrics
      httpRequestsTotal = promauto.NewCounterVec(
          prometheus.CounterOpts{
              Name: "lmmc_http_requests_total",
              Help: "Total number of HTTP requests",
          },
          []string{"method", "endpoint", "status"},
      )
      
      httpDuration = promauto.NewHistogramVec(
          prometheus.HistogramOpts{
              Name:    "lmmc_http_duration_seconds",
              Help:    "HTTP request duration in seconds",
              Buckets: prometheus.DefBuckets,
          },
          []string{"method", "endpoint"},
      )
      
      // WebSocket metrics
      wsConnectionsActive = promauto.NewGauge(
          prometheus.GaugeOpts{
              Name: "lmmc_websocket_connections_active",
              Help: "Number of active WebSocket connections",
          },
      )
      
      wsMessagesTotal = promauto.NewCounterVec(
          prometheus.CounterOpts{
              Name: "lmmc_websocket_messages_total",
              Help: "Total WebSocket messages",
          },
          []string{"direction", "type"},
      )
      
      // AI metrics
      aiRequestsTotal = promauto.NewCounterVec(
          prometheus.CounterOpts{
              Name: "lmmc_ai_requests_total",
              Help: "Total AI API requests",
          },
          []string{"model", "operation", "status"},
      )
      
      aiTokensUsed = promauto.NewCounterVec(
          prometheus.CounterOpts{
              Name: "lmmc_ai_tokens_used_total",
              Help: "Total AI tokens consumed",
          },
          []string{"model", "type"}, // type: prompt, completion
      )
      
      aiResponseTime = promauto.NewHistogramVec(
          prometheus.HistogramOpts{
              Name:    "lmmc_ai_response_seconds",
              Help:    "AI API response time",
              Buckets: prometheus.ExponentialBuckets(0.1, 2, 10),
          },
          []string{"model", "operation"},
      )
      
      // Pattern detection metrics
      patternsDetected = promauto.NewCounterVec(
          prometheus.CounterOpts{
              Name: "lmmc_patterns_detected_total",
              Help: "Total patterns detected",
          },
          []string{"type", "repository"},
      )
      
      patternConfidence = promauto.NewHistogram(
          prometheus.HistogramOpts{
              Name:    "lmmc_pattern_confidence",
              Help:    "Pattern detection confidence scores",
              Buckets: prometheus.LinearBuckets(0, 0.1, 11), // 0 to 1
          },
      )
      
      // Storage metrics
      qdrantOperations = promauto.NewCounterVec(
          prometheus.CounterOpts{
              Name: "lmmc_qdrant_operations_total",
              Help: "Qdrant operations",
          },
          []string{"operation", "status"},
      )
      
      qdrantLatency = promauto.NewHistogramVec(
          prometheus.HistogramOpts{
              Name:    "lmmc_qdrant_latency_seconds",
              Help:    "Qdrant operation latency",
              Buckets: prometheus.DefBuckets,
          },
          []string{"operation"},
      )
  )
  
  // Custom collectors
  type TaskCollector struct {
      taskStore storage.TaskStorage
  }
  
  func NewTaskCollector(taskStore storage.TaskStorage) *TaskCollector {
      return &TaskCollector{taskStore: taskStore}
  }
  
  func (c *TaskCollector) Describe(ch chan<- *prometheus.Desc) {
      prometheus.DescribeByCollect(c, ch)
  }
  
  func (c *TaskCollector) Collect(ch chan<- prometheus.Metric) {
      // Collect task statistics
      stats, err := c.taskStore.GetStatistics(context.Background())
      if err != nil {
          ch <- prometheus.NewInvalidMetric(
              prometheus.NewDesc("lmmc_task_collector_error", "Task collector error", nil, nil),
              err,
          )
          return
      }
      
      // Task age distribution
      ch <- prometheus.MustNewConstMetric(
          prometheus.NewDesc(
              "lmmc_task_age_days",
              "Age of tasks in days",
              []string{"percentile"},
              nil,
          ),
          prometheus.GaugeValue,
          stats.AgeP50,
          "50",
      )
      
      ch <- prometheus.MustNewConstMetric(
          prometheus.NewDesc(
              "lmmc_task_age_days",
              "Age of tasks in days",
              []string{"percentile"},
              nil,
          ),
          prometheus.GaugeValue,
          stats.AgeP95,
          "95",
      )
      
      ch <- prometheus.MustNewConstMetric(
          prometheus.NewDesc(
              "lmmc_task_age_days",
              "Age of tasks in days",
              []string{"percentile"},
              nil,
          ),
          prometheus.GaugeValue,
          stats.AgeP99,
          "99",
      )
  }
  
  // Middleware for HTTP metrics
  func PrometheusMiddleware(next http.Handler) http.Handler {
      return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
          timer := prometheus.NewTimer(httpDuration.WithLabelValues(
              r.Method,
              chi.RouteContext(r.Context()).RoutePattern(),
          ))
          
          wrapped := &responseWriter{ResponseWriter: w, statusCode: http.StatusOK}
          
          next.ServeHTTP(wrapped, r)
          
          timer.ObserveDuration()
          
          httpRequestsTotal.WithLabelValues(
              r.Method,
              chi.RouteContext(r.Context()).RoutePattern(),
              strconv.Itoa(wrapped.statusCode),
          ).Inc()
      })
  }
  
  // Business metrics
  type MetricsService struct {
      taskStore    storage.TaskStorage
      patternStore storage.PatternStorage
      logger       *slog.Logger
  }
  
  func (s *MetricsService) RecordTaskCreated(task *entities.Task) {
      tasksTotal.WithLabelValues(
          task.Repository,
          task.Priority,
          task.Status,
      ).Inc()
      
      activeTasksGauge.WithLabelValues(
          task.Repository,
          task.Status,
      ).Inc()
  }
  
  func (s *MetricsService) RecordTaskCompleted(task *entities.Task, duration time.Duration) {
      taskDuration.WithLabelValues(
          task.Repository,
          task.Priority,
      ).Observe(duration.Seconds())
      
      activeTasksGauge.WithLabelValues(
          task.Repository,
          "in_progress",
      ).Dec()
      
      activeTasksGauge.WithLabelValues(
          task.Repository,
          "completed",
      ).Inc()
  }
  
  func (s *MetricsService) RecordAIRequest(model, operation string, success bool, tokens int, duration time.Duration) {
      status := "success"
      if !success {
          status = "failure"
      }
      
      aiRequestsTotal.WithLabelValues(model, operation, status).Inc()
      aiTokensUsed.WithLabelValues(model, "total").Add(float64(tokens))
      aiResponseTime.WithLabelValues(model, operation).Observe(duration.Seconds())
  }
  
  // Health check with metrics
  type HealthChecker struct {
      checks map[string]HealthCheck
      mu     sync.RWMutex
  }
  
  type HealthCheck func() error
  
  type HealthStatus struct {
      Status     string                 `json:"status"`
      Checks     map[string]CheckResult `json:"checks"`
      Timestamp  time.Time              `json:"timestamp"`
      Version    string                 `json:"version"`
  }
  
  type CheckResult struct {
      Status   string        `json:"status"`
      Message  string        `json:"message,omitempty"`
      Duration time.Duration `json:"duration_ms"`
  }
  
  func (h *HealthChecker) CheckHealth() HealthStatus {
      h.mu.RLock()
      defer h.mu.RUnlock()
      
      status := HealthStatus{
          Status:    "healthy",
          Checks:    make(map[string]CheckResult),
          Timestamp: time.Now(),
          Version:   version.Version,
      }
      
      for name, check := range h.checks {
          start := time.Now()
          err := check()
          duration := time.Since(start)
          
          result := CheckResult{
              Status:   "healthy",
              Duration: duration,
          }
          
          if err != nil {
              result.Status = "unhealthy"
              result.Message = err.Error()
              status.Status = "unhealthy"
          }
          
          status.Checks[name] = result
      }
      
      return status
  }
  ```

  ```yaml
  # prometheus/prometheus.yml
  global:
    scrape_interval: 15s
    evaluation_interval: 15s
    external_labels:
      cluster: 'production'
      service: 'lmmc'
  
  rule_files:
    - /etc/prometheus/rules/*.yml
  
  alerting:
    alertmanagers:
      - static_configs:
          - targets:
              - alertmanager:9093
  
  scrape_configs:
    - job_name: 'lmmc-server'
      static_configs:
        - targets: ['lmmc-server:9080']
      metrics_path: '/metrics'
      
    - job_name: 'lmmc-cli'
      static_configs:
        - targets: ['localhost:9081']
      metrics_path: '/metrics'
      
    - job_name: 'qdrant'
      static_configs:
        - targets: ['qdrant:6333']
      metrics_path: '/metrics'
      
    - job_name: 'node-exporter'
      static_configs:
        - targets: ['node-exporter:9100']
  ```

  ```yaml
  # prometheus/rules/alerts.yml
  groups:
    - name: lmmc_alerts
      interval: 30s
      rules:
        - alert: HighErrorRate
          expr: |
            (
              sum(rate(lmmc_http_requests_total{status=~"5.."}[5m]))
              /
              sum(rate(lmmc_http_requests_total[5m]))
            ) > 0.05
          for: 5m
          labels:
            severity: critical
            team: platform
          annotations:
            summary: "High error rate detected"
            description: "Error rate is {{ $value | humanizePercentage }} over the last 5 minutes"
            
        - alert: HighAIAPILatency
          expr: |
            histogram_quantile(0.95,
              sum(rate(lmmc_ai_response_seconds_bucket[5m])) by (le, model)
            ) > 30
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "High AI API latency"
            description: "P95 latency for {{ $labels.model }} is {{ $value }}s"
            
        - alert: LowPatternConfidence
          expr: |
            histogram_quantile(0.5,
              sum(rate(lmmc_pattern_confidence_bucket[1h])) by (le)
            ) < 0.6
          for: 30m
          labels:
            severity: warning
          annotations:
            summary: "Low pattern detection confidence"
            description: "Median confidence score is {{ $value }}"
            
        - alert: QdrantConnectionFailure
          expr: |
            sum(rate(lmmc_qdrant_operations_total{status="error"}[5m])) > 10
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Qdrant connection failures"
            description: "{{ $value }} errors per second"
  ```

  ```json
  // grafana/dashboards/lmmc-overview.json
  {
    "dashboard": {
      "title": "LMMC System Overview",
      "panels": [
        {
          "title": "Request Rate",
          "targets": [
            {
              "expr": "sum(rate(lmmc_http_requests_total[5m])) by (method)"
            }
          ],
          "type": "graph"
        },
        {
          "title": "Error Rate",
          "targets": [
            {
              "expr": "sum(rate(lmmc_http_requests_total{status=~\"5..\"}[5m])) / sum(rate(lmmc_http_requests_total[5m]))"
            }
          ],
          "type": "stat"
        },
        {
          "title": "Active Tasks by Repository",
          "targets": [
            {
              "expr": "sum(lmmc_active_tasks) by (repository, status)"
            }
          ],
          "type": "graph"
        },
        {
          "title": "AI Token Usage",
          "targets": [
            {
              "expr": "sum(rate(lmmc_ai_tokens_used_total[1h])) by (model)"
            }
          ],
          "type": "bargauge"
        },
        {
          "title": "Pattern Detection Confidence",
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum(rate(lmmc_pattern_confidence_bucket[5m])) by (le))"
            }
          ],
          "type": "gauge"
        },
        {
          "title": "WebSocket Connections",
          "targets": [
            {
              "expr": "lmmc_websocket_connections_active"
            }
          ],
          "type": "stat"
        }
      ]
    }
  }
  ```

  ```yaml
  # docker-compose.monitoring.yml
  version: '3.8'
  
  services:
    prometheus:
      image: prom/prometheus:v2.45.0
      container_name: lmmc-prometheus
      command:
        - '--config.file=/etc/prometheus/prometheus.yml'
        - '--storage.tsdb.path=/prometheus'
        - '--web.console.libraries=/usr/share/prometheus/console_libraries'
        - '--web.console.templates=/usr/share/prometheus/consoles'
        - '--web.enable-lifecycle'
      volumes:
        - ./deployments/docker/prometheus:/etc/prometheus
        - prometheus_data:/prometheus
      ports:
        - "9090:9090"
      networks:
        - lmmc-network
      restart: unless-stopped
  
    grafana:
      image: grafana/grafana:10.0.0
      container_name: lmmc-grafana
      environment:
        - GF_SECURITY_ADMIN_PASSWORD=admin
        - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
      volumes:
        - ./deployments/docker/grafana/provisioning:/etc/grafana/provisioning
        - ./deployments/docker/grafana/dashboards:/var/lib/grafana/dashboards
        - grafana_data:/var/lib/grafana
      ports:
        - "3000:3000"
      networks:
        - lmmc-network
      restart: unless-stopped
      depends_on:
        - prometheus
  
    alertmanager:
      image: prom/alertmanager:v0.25.0
      container_name: lmmc-alertmanager
      command:
        - '--config.file=/etc/alertmanager/alertmanager.yml'
        - '--storage.path=/alertmanager'
      volumes:
        - ./deployments/docker/alertmanager:/etc/alertmanager
        - alertmanager_data:/alertmanager
      ports:
        - "9093:9093"
      networks:
        - lmmc-network
      restart: unless-stopped
  
    node-exporter:
      image: prom/node-exporter:v1.6.0
      container_name: lmmc-node-exporter
      command:
        - '--path.rootfs=/host'
        - '--path.procfs=/host/proc'
        - '--path.sysfs=/host/sys'
        - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
      volumes:
        - /proc:/host/proc:ro
        - /sys:/host/sys:ro
        - /:/rootfs:ro
      ports:
        - "9100:9100"
      networks:
        - lmmc-network
      restart: unless-stopped
  
  volumes:
    prometheus_data:
    grafana_data:
    alertmanager_data:
  
  networks:
    lmmc-network:
      external: true
  ```

- **Configuration Changes:** 
  - Add metrics endpoint to server
  - Configure Prometheus scraping
  - Setup Grafana data sources
  - Configure alerting rules

- **Dependencies:**
  - Prometheus client library
  - Docker and Docker Compose
  - Grafana provisioning

## 4. Acceptance Criteria
- **Functional Criteria:**
  - All key metrics are collected
  - Dashboards display real-time data
  - Alerts fire on defined conditions
  - Health checks report accurate status
  - Historical data is retained
  - Custom business metrics tracked
  - Performance metrics accurate
  
- **Technical Criteria:**
  - Metrics collection overhead <1% CPU
  - Prometheus scraping works reliably
  - Grafana dashboards load quickly
  - Alert rules evaluate correctly
  - Data retention configured properly
  
- **Integration Criteria:**
  - Server exposes metrics endpoint
  - CLI can export metrics
  - All services instrumented
  - Docker Compose setup works
  
- **Test Criteria:**
  - Metrics accuracy validated
  - Dashboard functionality tested
  - Alert rules trigger correctly
  - Load testing with metrics

## 5. Testing Requirements
- **Unit Tests:**
  - Metric instrumentation
  - Custom collectors
  - Health check logic
  - Alert rule validation
  
- **Integration Tests:**
  - Full monitoring stack
  - Metric collection flow
  - Alert firing and routing
  - Dashboard data accuracy
  
- **Manual Testing:**
  - Dashboard visual inspection
  - Alert notification testing
  - Performance impact measurement
  - Failure scenario testing
  
- **Test Data:**
  - Simulated metric data
  - Load test scenarios
  - Alert condition triggers

## 6. Definition of Done
- **Code Complete:** All services instrumented with metrics
- **Tests Passing:** Monitoring tests passing
- **Documentation Updated:** Metrics reference and runbooks created
- **Integration Verified:** Full monitoring stack operational
- **Review Approved:** SRE review of metrics and alerts

## 7. Dependencies and Blockers
- **Required Sub-Tasks:** None
- **External Dependencies:** Prometheus, Grafana, Docker
- **Environmental Requirements:** Docker environment
- **Potential Blockers:** Resource constraints for monitoring stack

## 8. Integration Notes
- **Component Interfaces:** Metrics endpoints on all services
- **Data Flow:** Services → Prometheus → Grafana/Alertmanager
- **Error Handling:** Graceful degradation without metrics
- **Configuration Impact:** New monitoring configuration section