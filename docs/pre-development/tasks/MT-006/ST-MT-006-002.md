# ST-006-02: Multi-Model AI Service Integration Layer

**Parent Task**: MT-006 - Server-Side AI Integration and HTTP API Foundation  
**Duration**: 4 hours  
**Type**: AI Integration

## Overview
Implement comprehensive AI service layer with multi-model support (Claude Sonnet 4, Perplexity Sonar Pro, OpenAI GPT-4o) including fallback routing and response caching.

## Scope
- Create AI service abstraction layer with multi-model support
- Implement AI client configurations for all three models
- Add intelligent fallback routing when primary model fails
- Create response caching system for AI optimization
- Implement AI request/response monitoring
- Add AI model performance tracking
- Setup AI cost optimization strategies

## Technical Requirements
- Abstracted AI service interface for all models
- Client implementations for Claude, Perplexity, and OpenAI APIs
- Fallback chain: Claude → Perplexity → OpenAI
- Response caching with configurable TTL
- Request/response logging for AI interactions
- Error handling for rate limits and API failures
- AI model performance metrics collection

## Acceptance Criteria
- [ ] AI service supports all three configured models
- [ ] Fallback routing activates within 5 seconds of primary failure
- [ ] Response caching reduces redundant API calls by >70%
- [ ] AI requests complete within 30 seconds timeout
- [ ] Rate limit handling prevents API key suspension
- [ ] AI response quality is consistent across models
- [ ] Cost tracking provides visibility into API usage
- [ ] Error handling provides clear failure reasons

## Files to Create/Modify
- `internal/ai/service.go` - AI service abstraction
- `internal/ai/claude_client.go` - Claude Sonnet 4 client
- `internal/ai/perplexity_client.go` - Perplexity Sonar Pro client
- `internal/ai/openai_client.go` - OpenAI GPT-4o client
- `internal/ai/fallback.go` - Fallback routing logic
- `internal/ai/cache.go` - Response caching
- `internal/ai/metrics.go` - AI performance metrics
- `configs/ai_models.yaml` - AI model configuration

## Dependencies
- AI model API credentials
- HTTP client libraries
- Caching infrastructure
- Metrics collection system

## Testing Requirements
- Unit tests for each AI client
- Integration tests with AI APIs
- Fallback scenario testing
- Cache effectiveness validation
- Rate limiting simulation

## Success Metrics
- AI response time <30s p95
- Fallback success rate >95%
- Cache hit rate >70%
- API error rate <5%