// Package storage provides search and retrieval implementation using Qdrant vector database
package storage

import (
	"context"
	"encoding/json"
	"fmt"
	"log/slog"
	"sort"
	"strings"
	"time"

	"github.com/qdrant/go-client/qdrant"

	"lerian-mcp-memory/internal/types"
)

// SearchStoreImpl implements SearchStore interface using Qdrant
type SearchStoreImpl struct {
	client         *qdrant.Client
	collectionName string
	logger         *slog.Logger
	contentStore   *ContentStoreImpl
	metrics        *StorageMetrics
}

// NewSearchStore creates a new search store with Qdrant backend
func NewSearchStore(client *qdrant.Client, collectionName string, logger *slog.Logger, contentStore *ContentStoreImpl) *SearchStoreImpl {
	if logger == nil {
		logger = slog.Default()
	}

	return &SearchStoreImpl{
		client:         client,
		collectionName: collectionName,
		logger:         logger,
		contentStore:   contentStore,
		metrics: &StorageMetrics{
			OperationCounts:  make(map[string]int64),
			AverageLatency:   make(map[string]float64),
			ErrorCounts:      make(map[string]int64),
			ConnectionStatus: "connected",
		},
	}
}

// Search performs vector similarity search with filtering
func (ss *SearchStoreImpl) Search(ctx context.Context, query *types.SearchQuery) (*types.SearchResults, error) {
	start := time.Now()
	defer ss.updateMetrics("search", start)

	if err := ss.validateSearchQuery(query); err != nil {
		ss.incrementErrorCount("search")
		return nil, fmt.Errorf("search query validation failed: %w", err)
	}

	// For vector search, we need embeddings for the query
	// In a real implementation, this would be generated by an embedding service
	if len(query.Query) == 0 {
		ss.incrementErrorCount("search")
		return nil, fmt.Errorf("search query cannot be empty")
	}

	// Set defaults
	limit := query.Limit
	if limit <= 0 {
		limit = 20
	}
	offset := query.Offset
	if offset < 0 {
		offset = 0
	}
	minRelevance := query.MinRelevance
	if minRelevance <= 0 {
		minRelevance = 0.1 // Default minimum relevance threshold
	}

	// Build Qdrant filter based on query criteria
	filter := ss.buildQdrantFilter(query)

	// NOTE: This is a placeholder for vector search
	// In production, you would:
	// 1. Generate embeddings for query.Query using OpenAI/other embedding service
	// 2. Use those embeddings for vector search
	// For now, we'll do a simple payload-based search

	// Perform search using scroll (since we don't have query embeddings yet)
	searchResults, err := ss.performScrollSearch(ctx, query, filter, limit, offset)
	if err != nil {
		ss.incrementErrorCount("search")
		return nil, fmt.Errorf("failed to perform search: %w", err)
	}

	// Filter by relevance if using vector search
	filteredResults := ss.filterByRelevance(searchResults, minRelevance)

	// Sort results
	ss.sortResults(filteredResults, query.SortBy, query.SortOrder)

	// Calculate pagination
	page := 0
	if limit > 0 {
		page = offset / limit
	}

	results := &types.SearchResults{
		Results:      filteredResults,
		Total:        len(filteredResults),
		Page:         page,
		PerPage:      limit,
		Query:        query.Query,
		Duration:     time.Since(start),
		MaxRelevance: ss.calculateMaxRelevance(filteredResults),
	}

	ss.logger.Debug("search completed",
		slog.String("query", query.Query),
		slog.String("project_id", string(query.ProjectID)),
		slog.Int("results", len(filteredResults)),
		slog.Duration("duration", results.Duration))

	return results, nil
}

// FindSimilar finds content similar to the given text using vector similarity
func (ss *SearchStoreImpl) FindSimilar(ctx context.Context, content string, projectID types.ProjectID, sessionID types.SessionID) ([]*types.Content, error) {
	start := time.Now()
	defer ss.updateMetrics("find_similar", start)

	if err := projectID.Validate(); err != nil {
		ss.incrementErrorCount("find_similar")
		return nil, fmt.Errorf("invalid project ID: %w", err)
	}

	if strings.TrimSpace(content) == "" {
		ss.incrementErrorCount("find_similar")
		return nil, fmt.Errorf("content for similarity search cannot be empty")
	}

	// NOTE: In production, this would:
	// 1. Generate embeddings for the input content
	// 2. Perform vector similarity search using those embeddings
	// For now, we'll use text-based similarity as a placeholder

	filter := &qdrant.Filter{
		Must: []*qdrant.Condition{
			{
				ConditionOneOf: &qdrant.Condition_Field{
					Field: &qdrant.FieldCondition{
						Key: "project_id",
						Match: &qdrant.Match{
							MatchValue: &qdrant.Match_Keyword{Keyword: string(projectID)},
						},
					},
				},
			},
		},
	}

	// Add session filter if provided
	if sessionID != "" {
		sessionCondition := &qdrant.Condition{
			ConditionOneOf: &qdrant.Condition_Field{
				Field: &qdrant.FieldCondition{
					Key: "session_id",
					Match: &qdrant.Match{
						MatchValue: &qdrant.Match_Keyword{Keyword: string(sessionID)},
					},
				},
			},
		}
		filter.Must = append(filter.Must, sessionCondition)
	}

	// Perform scroll to get all matching content
	scrollResult, err := ss.client.Scroll(ctx, &qdrant.ScrollPoints{
		CollectionName: ss.collectionName,
		Filter:         filter,
		Limit:          100, // Reasonable limit for similarity search
		WithPayload: &qdrant.WithPayloadSelector{
			SelectorOptions: &qdrant.WithPayloadSelector_Enable{Enable: true},
		},
		WithVectors: &qdrant.WithVectorsSelector{
			SelectorOptions: &qdrant.WithVectorsSelector_Enable{Enable: true},
		},
	})
	if err != nil {
		ss.incrementErrorCount("find_similar")
		return nil, fmt.Errorf("failed to scroll points for similarity search: %w", err)
	}

	// Convert points to content and calculate similarity
	var similarContent []*types.Content
	queryWords := strings.Fields(strings.ToLower(content))

	for _, point := range scrollResult.Result {
		contentObj, err := ss.contentStore.pointToContent(point)
		if err != nil {
			ss.logger.Warn("failed to convert point to content",
				slog.String("error", err.Error()))
			continue
		}

		// Calculate text-based similarity (placeholder for vector similarity)
		similarity := ss.calculateTextSimilarity(content, contentObj.Content)
		if similarity > 0.1 { // Minimum similarity threshold
			similarContent = append(similarContent, contentObj)
		}
	}

	// Sort by similarity (in a real implementation, this would be vector similarity score)
	sort.Slice(similarContent, func(i, j int) bool {
		simI := ss.calculateTextSimilarity(content, similarContent[i].Content)
		simJ := ss.calculateTextSimilarity(content, similarContent[j].Content)
		return simI > simJ
	})

	// Limit results
	const maxSimilarResults = 10
	if len(similarContent) > maxSimilarResults {
		similarContent = similarContent[:maxSimilarResults]
	}

	ss.logger.Debug("similarity search completed",
		slog.String("project_id", string(projectID)),
		slog.Int("results", len(similarContent)))

	return similarContent, nil
}

// GetByProject retrieves content within a project with optional filters
func (ss *SearchStoreImpl) GetByProject(ctx context.Context, projectID types.ProjectID, filters *types.Filters) ([]*types.Content, error) {
	start := time.Now()
	defer ss.updateMetrics("get_by_project", start)

	if err := projectID.Validate(); err != nil {
		ss.incrementErrorCount("get_by_project")
		return nil, fmt.Errorf("invalid project ID: %w", err)
	}

	// Build base filter for project
	filter := &qdrant.Filter{
		Must: []*qdrant.Condition{
			{
				ConditionOneOf: &qdrant.Condition_Field{
					Field: &qdrant.FieldCondition{
						Key: "project_id",
						Match: &qdrant.Match{
							MatchValue: &qdrant.Match_Keyword{Keyword: string(projectID)},
						},
					},
				},
			},
		},
	}

	// Apply additional filters
	if filters != nil {
		ss.applyFiltersToQdrantFilter(filter, filters)
	}

	// Perform scroll to get all matching content
	var allContent []*types.Content
	var nextPageOffset *qdrant.PointId

	for {
		scrollResult, err := ss.client.Scroll(ctx, &qdrant.ScrollPoints{
			CollectionName: ss.collectionName,
			Filter:         filter,
			Limit:          100,
			Offset:         nextPageOffset,
			WithPayload: &qdrant.WithPayloadSelector{
				SelectorOptions: &qdrant.WithPayloadSelector_Enable{Enable: true},
			},
			WithVectors: &qdrant.WithVectorsSelector{
				SelectorOptions: &qdrant.WithVectorsSelector_Enable{Enable: true},
			},
		})
		if err != nil {
			ss.incrementErrorCount("get_by_project")
			return nil, fmt.Errorf("failed to scroll points: %w", err)
		}

		// Convert points to content
		for _, point := range scrollResult.Result {
			contentObj, err := ss.contentStore.pointToContent(point)
			if err != nil {
				ss.logger.Warn("failed to convert point to content",
					slog.String("error", err.Error()))
				continue
			}
			allContent = append(allContent, contentObj)
		}

		// Check if there are more results
		if scrollResult.NextPageOffset == nil {
			break
		}
		nextPageOffset = scrollResult.NextPageOffset
	}

	ss.logger.Debug("get by project completed",
		slog.String("project_id", string(projectID)),
		slog.Int("results", len(allContent)))

	return allContent, nil
}

// GetBySession retrieves content within a session
func (ss *SearchStoreImpl) GetBySession(ctx context.Context, projectID types.ProjectID, sessionID types.SessionID, filters *types.Filters) ([]*types.Content, error) {
	start := time.Now()
	defer ss.updateMetrics("get_by_session", start)

	if err := projectID.Validate(); err != nil {
		ss.incrementErrorCount("get_by_session")
		return nil, fmt.Errorf("invalid project ID: %w", err)
	}

	if sessionID == "" {
		ss.incrementErrorCount("get_by_session")
		return nil, fmt.Errorf("session ID cannot be empty")
	}

	// Build filter for project and session
	filter := &qdrant.Filter{
		Must: []*qdrant.Condition{
			{
				ConditionOneOf: &qdrant.Condition_Field{
					Field: &qdrant.FieldCondition{
						Key: "project_id",
						Match: &qdrant.Match{
							MatchValue: &qdrant.Match_Keyword{Keyword: string(projectID)},
						},
					},
				},
			},
			{
				ConditionOneOf: &qdrant.Condition_Field{
					Field: &qdrant.FieldCondition{
						Key: "session_id",
						Match: &qdrant.Match{
							MatchValue: &qdrant.Match_Keyword{Keyword: string(sessionID)},
						},
					},
				},
			},
		},
	}

	// Apply additional filters
	if filters != nil {
		ss.applyFiltersToQdrantFilter(filter, filters)
	}

	// Get content using scroll
	var sessionContent []*types.Content
	var nextPageOffset *qdrant.PointId

	for {
		scrollResult, err := ss.client.Scroll(ctx, &qdrant.ScrollPoints{
			CollectionName: ss.collectionName,
			Filter:         filter,
			Limit:          100,
			Offset:         nextPageOffset,
			WithPayload: &qdrant.WithPayloadSelector{
				SelectorOptions: &qdrant.WithPayloadSelector_Enable{Enable: true},
			},
			WithVectors: &qdrant.WithVectorsSelector{
				SelectorOptions: &qdrant.WithVectorsSelector_Enable{Enable: true},
			},
		})
		if err != nil {
			ss.incrementErrorCount("get_by_session")
			return nil, fmt.Errorf("failed to scroll session content: %w", err)
		}

		// Convert points to content
		for _, point := range scrollResult.Result {
			contentObj, err := ss.contentStore.pointToContent(point)
			if err != nil {
				ss.logger.Warn("failed to convert point to content",
					slog.String("error", err.Error()))
				continue
			}
			sessionContent = append(sessionContent, contentObj)
		}

		// Check if there are more results
		if scrollResult.NextPageOffset == nil {
			break
		}
		nextPageOffset = scrollResult.NextPageOffset
	}

	ss.logger.Debug("get by session completed",
		slog.String("project_id", string(projectID)),
		slog.String("session_id", string(sessionID)),
		slog.Int("results", len(sessionContent)))

	return sessionContent, nil
}

// GetHistory retrieves content history (placeholder implementation)
func (ss *SearchStoreImpl) GetHistory(ctx context.Context, projectID types.ProjectID, contentID string) ([]*types.ContentVersion, error) {
	start := time.Now()
	defer ss.updateMetrics("get_history", start)

	if err := projectID.Validate(); err != nil {
		ss.incrementErrorCount("get_history")
		return nil, fmt.Errorf("invalid project ID: %w", err)
	}

	if contentID == "" {
		ss.incrementErrorCount("get_history")
		return nil, fmt.Errorf("content ID cannot be empty")
	}

	// TODO: Implement proper history tracking
	// For now, return empty history as this requires additional storage design
	ss.logger.Debug("history retrieval not yet implemented",
		slog.String("project_id", string(projectID)),
		slog.String("content_id", contentID))

	return []*types.ContentVersion{}, nil
}

// Helper methods

func (ss *SearchStoreImpl) validateSearchQuery(query *types.SearchQuery) error {
	if query == nil {
		return fmt.Errorf("search query cannot be nil")
	}
	if err := query.ProjectID.Validate(); err != nil {
		return fmt.Errorf("invalid project ID: %w", err)
	}
	if query.Limit < 0 {
		return fmt.Errorf("limit cannot be negative")
	}
	if query.Offset < 0 {
		return fmt.Errorf("offset cannot be negative")
	}
	if query.MinRelevance < 0 || query.MinRelevance > 1 {
		return fmt.Errorf("min_relevance must be between 0 and 1")
	}
	return nil
}

func (ss *SearchStoreImpl) buildQdrantFilter(query *types.SearchQuery) *qdrant.Filter {
	filter := &qdrant.Filter{
		Must: []*qdrant.Condition{
			{
				ConditionOneOf: &qdrant.Condition_Field{
					Field: &qdrant.FieldCondition{
						Key: "project_id",
						Match: &qdrant.Match{
							MatchValue: &qdrant.Match_Keyword{Keyword: string(query.ProjectID)},
						},
					},
				},
			},
		},
	}

	// Add session filter if provided
	if query.SessionID != "" {
		sessionCondition := &qdrant.Condition{
			ConditionOneOf: &qdrant.Condition_Field{
				Field: &qdrant.FieldCondition{
					Key: "session_id",
					Match: &qdrant.Match{
						MatchValue: &qdrant.Match_Keyword{Keyword: string(query.SessionID)},
					},
				},
			},
		}
		filter.Must = append(filter.Must, sessionCondition)
	}

	// Add type filters
	if len(query.Types) > 0 {
		typeConditions := make([]*qdrant.Condition, len(query.Types))
		for i, contentType := range query.Types {
			typeConditions[i] = &qdrant.Condition{
				ConditionOneOf: &qdrant.Condition_Field{
					Field: &qdrant.FieldCondition{
						Key: "type",
						Match: &qdrant.Match{
							MatchValue: &qdrant.Match_Keyword{Keyword: contentType},
						},
					},
				},
			}
		}
		if len(typeConditions) == 1 {
			filter.Must = append(filter.Must, typeConditions[0])
		} else {
			filter.Must = append(filter.Must, &qdrant.Condition{
				ConditionOneOf: &qdrant.Condition_Filter{
					Filter: &qdrant.Filter{Should: typeConditions},
				},
			})
		}
	}

	// Apply additional filters from Filters struct
	if query.Filters != nil {
		ss.applyFiltersToQdrantFilter(filter, query.Filters)
	}

	return filter
}

func (ss *SearchStoreImpl) applyFiltersToQdrantFilter(filter *qdrant.Filter, filters *types.Filters) {
	// Add type filters
	if len(filters.Types) > 0 {
		typeConditions := make([]*qdrant.Condition, len(filters.Types))
		for i, contentType := range filters.Types {
			typeConditions[i] = &qdrant.Condition{
				ConditionOneOf: &qdrant.Condition_Field{
					Field: &qdrant.FieldCondition{
						Key: "type",
						Match: &qdrant.Match{
							MatchValue: &qdrant.Match_Keyword{Keyword: contentType},
						},
					},
				},
			}
		}
		if len(typeConditions) == 1 {
			filter.Must = append(filter.Must, typeConditions[0])
		} else {
			filter.Must = append(filter.Must, &qdrant.Condition{
				ConditionOneOf: &qdrant.Condition_Filter{
					Filter: &qdrant.Filter{Should: typeConditions},
				},
			})
		}
	}

	// Add timestamp filters
	if filters.CreatedAfter != nil {
		filter.Must = append(filter.Must, &qdrant.Condition{
			ConditionOneOf: &qdrant.Condition_Field{
				Field: &qdrant.FieldCondition{
					Key: "created_at",
					Range: &qdrant.Range{
						Gte: float64(filters.CreatedAfter.Unix()),
					},
				},
			},
		})
	}

	if filters.CreatedBefore != nil {
		filter.Must = append(filter.Must, &qdrant.Condition{
			ConditionOneOf: &qdrant.Condition_Field{
				Field: &qdrant.FieldCondition{
					Key: "created_at",
					Range: &qdrant.Range{
						Lte: float64(filters.CreatedBefore.Unix()),
					},
				},
			},
		})
	}

	// Add quality filters
	if filters.MinQuality != nil {
		filter.Must = append(filter.Must, &qdrant.Condition{
			ConditionOneOf: &qdrant.Condition_Field{
				Field: &qdrant.FieldCondition{
					Key: "quality",
					Range: &qdrant.Range{
						Gte: *filters.MinQuality,
					},
				},
			},
		})
	}

	if filters.MinConfidence != nil {
		filter.Must = append(filter.Must, &qdrant.Condition{
			ConditionOneOf: &qdrant.Condition_Field{
				Field: &qdrant.FieldCondition{
					Key: "confidence",
					Range: &qdrant.Range{
						Gte: *filters.MinConfidence,
					},
				},
			},
		})
	}
}

func (ss *SearchStoreImpl) performScrollSearch(ctx context.Context, query *types.SearchQuery, filter *qdrant.Filter, limit, offset int) ([]*types.SearchResult, error) {
	var allResults []*types.SearchResult
	var nextPageOffset *qdrant.PointId
	collected := 0
	skipped := 0

	for {
		scrollResult, err := ss.client.Scroll(ctx, &qdrant.ScrollPoints{
			CollectionName: ss.collectionName,
			Filter:         filter,
			Limit:          100, // Scroll in chunks
			Offset:         nextPageOffset,
			WithPayload: &qdrant.WithPayloadSelector{
				SelectorOptions: &qdrant.WithPayloadSelector_Enable{Enable: true},
			},
			WithVectors: &qdrant.WithVectorsSelector{
				SelectorOptions: &qdrant.WithVectorsSelector_Enable{Enable: true},
			},
		})
		if err != nil {
			return nil, fmt.Errorf("failed to scroll points: %w", err)
		}

		// Convert points to search results
		for _, point := range scrollResult.Result {
			// Skip until we reach the offset
			if skipped < offset {
				skipped++
				continue
			}

			// Stop if we've collected enough results
			if limit > 0 && collected >= limit {
				return allResults, nil
			}

			contentObj, err := ss.contentStore.pointToContent(point)
			if err != nil {
				ss.logger.Warn("failed to convert point to content",
					slog.String("error", err.Error()))
				continue
			}

			// Calculate relevance (placeholder - would use vector similarity in production)
			relevance := ss.calculateTextRelevance(query.Query, contentObj.Content)

			searchResult := &types.SearchResult{
				Content:     contentObj,
				Relevance:   relevance,
				Highlights:  ss.generateHighlights(query.Query, contentObj.Content),
				Context:     ss.generateContext(contentObj.Content, query.Query),
				Explanation: fmt.Sprintf("Matched based on text similarity (relevance: %.2f)", relevance),
			}

			allResults = append(allResults, searchResult)
			collected++
		}

		// Check if there are more results and we haven't reached our limit
		if scrollResult.NextPageOffset == nil || (limit > 0 && collected >= limit) {
			break
		}
		nextPageOffset = scrollResult.NextPageOffset
	}

	return allResults, nil
}

func (ss *SearchStoreImpl) filterByRelevance(results []*types.SearchResult, minRelevance float64) []*types.SearchResult {
	if minRelevance <= 0 {
		return results
	}

	filtered := make([]*types.SearchResult, 0, len(results))
	for _, result := range results {
		if result.Relevance >= minRelevance {
			filtered = append(filtered, result)
		}
	}
	return filtered
}

func (ss *SearchStoreImpl) sortResults(results []*types.SearchResult, sortBy, sortOrder string) {
	if len(results) <= 1 {
		return
	}

	switch sortBy {
	case "created_at":
		sort.Slice(results, func(i, j int) bool {
			if sortOrder == "asc" {
				return results[i].Content.CreatedAt.Before(results[j].Content.CreatedAt)
			}
			return results[i].Content.CreatedAt.After(results[j].Content.CreatedAt)
		})
	case "updated_at":
		sort.Slice(results, func(i, j int) bool {
			if sortOrder == "asc" {
				return results[i].Content.UpdatedAt.Before(results[j].Content.UpdatedAt)
			}
			return results[i].Content.UpdatedAt.After(results[j].Content.UpdatedAt)
		})
	default: // "relevance" or unspecified
		sort.Slice(results, func(i, j int) bool {
			if sortOrder == "asc" {
				return results[i].Relevance < results[j].Relevance
			}
			return results[i].Relevance > results[j].Relevance
		})
	}
}

func (ss *SearchStoreImpl) calculateMaxRelevance(results []*types.SearchResult) float64 {
	if len(results) == 0 {
		return 0.0
	}

	maxRelevance := 0.0
	for _, result := range results {
		if result.Relevance > maxRelevance {
			maxRelevance = result.Relevance
		}
	}
	return maxRelevance
}

func (ss *SearchStoreImpl) calculateTextSimilarity(text1, text2 string) float64 {
	// Simple text similarity calculation (placeholder for vector similarity)
	words1 := strings.Fields(strings.ToLower(text1))
	words2 := strings.Fields(strings.ToLower(text2))

	if len(words1) == 0 || len(words2) == 0 {
		return 0.0
	}

	// Create word sets
	set1 := make(map[string]bool)
	set2 := make(map[string]bool)

	for _, word := range words1 {
		set1[word] = true
	}
	for _, word := range words2 {
		set2[word] = true
	}

	// Calculate intersection
	intersection := 0
	for word := range set1 {
		if set2[word] {
			intersection++
		}
	}

	// Calculate union
	union := len(set1) + len(set2) - intersection

	if union == 0 {
		return 0.0
	}

	return float64(intersection) / float64(union)
}

func (ss *SearchStoreImpl) calculateTextRelevance(query, content string) float64 {
	if strings.TrimSpace(query) == "" || strings.TrimSpace(content) == "" {
		return 0.0
	}

	queryLower := strings.ToLower(query)
	contentLower := strings.ToLower(content)

	// Exact match gets highest relevance
	if strings.Contains(contentLower, queryLower) {
		return 0.9
	}

	// Calculate word overlap
	return ss.calculateTextSimilarity(query, content)
}

func (ss *SearchStoreImpl) generateHighlights(query, content string) []string {
	if strings.TrimSpace(query) == "" {
		return nil
	}

	queryWords := strings.Fields(strings.ToLower(query))
	contentWords := strings.Fields(content)

	var highlights []string
	for i, word := range contentWords {
		wordLower := strings.ToLower(word)
		for _, queryWord := range queryWords {
			if strings.Contains(wordLower, queryWord) {
				// Create context around the highlight
				start := i - 2
				end := i + 3
				if start < 0 {
					start = 0
				}
				if end > len(contentWords) {
					end = len(contentWords)
				}

				highlightContext := strings.Join(contentWords[start:end], " ")
				highlights = append(highlights, highlightContext)
				break
			}
		}
	}

	return highlights
}

func (ss *SearchStoreImpl) generateContext(content, query string) string {
	const maxContextLength = 200

	if len(content) <= maxContextLength {
		return content
	}

	queryLower := strings.ToLower(query)
	contentLower := strings.ToLower(content)

	// Find the best position to show context
	if pos := strings.Index(contentLower, queryLower); pos != -1 {
		start := pos - 50
		if start < 0 {
			start = 0
		}

		end := start + maxContextLength
		if end > len(content) {
			end = len(content)
		}

		context := content[start:end]
		if start > 0 {
			context = "..." + context
		}
		if end < len(content) {
			context = context + "..."
		}

		return context
	}

	// If no direct match, return beginning of content
	if len(content) > maxContextLength {
		return content[:maxContextLength] + "..."
	}

	return content
}

func (ss *SearchStoreImpl) updateMetrics(operation string, start time.Time) {
	duration := time.Since(start)
	ss.metrics.OperationCounts[operation]++

	// Update average latency (simple moving average)
	current := ss.metrics.AverageLatency[operation]
	count := ss.metrics.OperationCounts[operation]
	ss.metrics.AverageLatency[operation] = (current*float64(count-1) + duration.Seconds()) / float64(count)
}

func (ss *SearchStoreImpl) incrementErrorCount(operation string) {
	ss.metrics.ErrorCounts[operation]++
}

// GetMetrics returns current search metrics
func (ss *SearchStoreImpl) GetMetrics() *StorageMetrics {
	return ss.metrics
}
